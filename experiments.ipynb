{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "55c2eee040cd7a7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T21:51:11.134159Z",
     "start_time": "2024-12-18T21:51:11.108716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as TorchF\n",
    "from torch import Tensor as TorchTensor\n",
    "\n",
    "# mini_torch imports\n",
    "from nn import Linear as MiniLinear, Module as MiniModule, MSELoss as MiniMSELoss\n",
    "from optim import Adam as MiniAdam\n",
    "from tensor import Tensor as MiniTensor\n",
    "import nn as mini_nn\n",
    "import nn.functional as MiniF\n",
    "\n",
    "# helper functions\n",
    "def gradients_are_equal(torch_tensor: TorchTensor, mini_tensor: MiniTensor):\n",
    "    print(np.all(torch_tensor.grad.detach().numpy()==mini_tensor.grad))"
   ],
   "id": "58df59fdbb24d3ad",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Build a Simple MLP (with MiniTorch API)",
   "id": "53c290f969bf96f2"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:51:11.470958Z",
     "start_time": "2024-12-18T21:51:11.388317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(MiniModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = MiniLinear(3, 3)\n",
    "        self.linear_2 = MiniLinear(3, 6)\n",
    "        self.linear_3 = MiniLinear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = MiniF.tanh(self.linear_1(x))\n",
    "        x = MiniF.tanh(self.linear_2(x))\n",
    "        x = MiniF.tanh(self.linear_3(x))\n",
    "        return x\n",
    "    \n",
    "model = MLP()\n",
    "optim = MiniAdam(model.parameters())\n",
    "criterion = MiniMSELoss()\n",
    "\n",
    "\n",
    "X = MiniTensor([[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]])\n",
    "Y = MiniTensor([[1.0], [-1.0], [-1.0], [1.0]])\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_hat = model(X)\n",
    "    loss = criterion(y_hat, Y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "    \n",
    "print('\\nModel architecture: \\n',model)\n",
    "print('\\n state dict:\\n' ,model.state_dict())"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9937796592712402\n",
      "Epoch 1, Loss: 1.9923006296157837\n",
      "Epoch 2, Loss: 1.9903831481933594\n",
      "Epoch 3, Loss: 1.9878700971603394\n",
      "Epoch 4, Loss: 1.984545350074768\n",
      "Epoch 5, Loss: 1.9801126718521118\n",
      "Epoch 6, Loss: 1.9741649627685547\n",
      "Epoch 7, Loss: 1.9661449193954468\n",
      "Epoch 8, Loss: 1.9552971124649048\n",
      "Epoch 9, Loss: 1.9406157732009888\n",
      "Epoch 10, Loss: 1.9207981824874878\n",
      "Epoch 11, Loss: 1.8942272663116455\n",
      "Epoch 12, Loss: 1.8590224981307983\n",
      "Epoch 13, Loss: 1.8132188320159912\n",
      "Epoch 14, Loss: 1.7551205158233643\n",
      "Epoch 15, Loss: 1.6837944984436035\n",
      "Epoch 16, Loss: 1.5995620489120483\n",
      "Epoch 17, Loss: 1.5047229528427124\n",
      "Epoch 18, Loss: 1.4042599201202393\n",
      "Epoch 19, Loss: 1.3052639961242676\n",
      "Epoch 20, Loss: 1.2151246070861816\n",
      "Epoch 21, Loss: 1.139133095741272\n",
      "Epoch 22, Loss: 1.079049825668335\n",
      "Epoch 23, Loss: 1.0334875583648682\n",
      "Epoch 24, Loss: 0.9993932247161865\n",
      "Epoch 25, Loss: 0.9734480381011963\n",
      "Epoch 26, Loss: 0.9528200030326843\n",
      "Epoch 27, Loss: 0.935355544090271\n",
      "Epoch 28, Loss: 0.9194931983947754\n",
      "Epoch 29, Loss: 0.9041027426719666\n",
      "Epoch 30, Loss: 0.8883406519889832\n",
      "Epoch 31, Loss: 0.8715445399284363\n",
      "Epoch 32, Loss: 0.8531667590141296\n",
      "Epoch 33, Loss: 0.8327415585517883\n",
      "Epoch 34, Loss: 0.8098825812339783\n",
      "Epoch 35, Loss: 0.7843105792999268\n",
      "Epoch 36, Loss: 0.755915105342865\n",
      "Epoch 37, Loss: 0.7248521447181702\n",
      "Epoch 38, Loss: 0.691675066947937\n",
      "Epoch 39, Loss: 0.6574773788452148\n",
      "Epoch 40, Loss: 0.623989999294281\n",
      "Epoch 41, Loss: 0.5935224890708923\n",
      "Epoch 42, Loss: 0.5685887932777405\n",
      "Epoch 43, Loss: 0.5510941743850708\n",
      "Epoch 44, Loss: 0.5412408113479614\n",
      "Epoch 45, Loss: 0.536846935749054\n",
      "Epoch 46, Loss: 0.533932626247406\n",
      "Epoch 47, Loss: 0.5283774733543396\n",
      "Epoch 48, Loss: 0.5173463225364685\n",
      "Epoch 49, Loss: 0.49970608949661255\n",
      "Epoch 50, Loss: 0.47572481632232666\n",
      "Epoch 51, Loss: 0.4465642273426056\n",
      "Epoch 52, Loss: 0.4138190746307373\n",
      "Epoch 53, Loss: 0.3791576027870178\n",
      "Epoch 54, Loss: 0.344062864780426\n",
      "Epoch 55, Loss: 0.30967751145362854\n",
      "Epoch 56, Loss: 0.27675753831863403\n",
      "Epoch 57, Loss: 0.24572338163852692\n",
      "Epoch 58, Loss: 0.21677249670028687\n",
      "Epoch 59, Loss: 0.19000005722045898\n",
      "Epoch 60, Loss: 0.16548220813274384\n",
      "Epoch 61, Loss: 0.14330099523067474\n",
      "Epoch 62, Loss: 0.12352222204208374\n",
      "Epoch 63, Loss: 0.10615570843219757\n",
      "Epoch 64, Loss: 0.09112770855426788\n",
      "Epoch 65, Loss: 0.07827980816364288\n",
      "Epoch 66, Loss: 0.06739084422588348\n",
      "Epoch 67, Loss: 0.05820883437991142\n",
      "Epoch 68, Loss: 0.05048052594065666\n",
      "Epoch 69, Loss: 0.0439717099070549\n",
      "Epoch 70, Loss: 0.03847755491733551\n",
      "Epoch 71, Loss: 0.03382524475455284\n",
      "Epoch 72, Loss: 0.029872005805373192\n",
      "Epoch 73, Loss: 0.026501037180423737\n",
      "Epoch 74, Loss: 0.023617016151547432\n",
      "Epoch 75, Loss: 0.02114192582666874\n",
      "Epoch 76, Loss: 0.019011590629816055\n",
      "Epoch 77, Loss: 0.017172889783978462\n",
      "Epoch 78, Loss: 0.015581601299345493\n",
      "Epoch 79, Loss: 0.014200720004737377\n",
      "Epoch 80, Loss: 0.012999165803194046\n",
      "Epoch 81, Loss: 0.011950747109949589\n",
      "Epoch 82, Loss: 0.011033345013856888\n",
      "Epoch 83, Loss: 0.010228246450424194\n",
      "Epoch 84, Loss: 0.009519597515463829\n",
      "Epoch 85, Loss: 0.008893945254385471\n",
      "Epoch 86, Loss: 0.008339866064488888\n",
      "Epoch 87, Loss: 0.007847641594707966\n",
      "Epoch 88, Loss: 0.0074089947156608105\n",
      "Epoch 89, Loss: 0.0070168632082641125\n",
      "Epoch 90, Loss: 0.006665213033556938\n",
      "Epoch 91, Loss: 0.0063488781452178955\n",
      "Epoch 92, Loss: 0.006063428241759539\n",
      "Epoch 93, Loss: 0.005805053282529116\n",
      "Epoch 94, Loss: 0.005570473615080118\n",
      "Epoch 95, Loss: 0.0053568570874631405\n",
      "Epoch 96, Loss: 0.005161753855645657\n",
      "Epoch 97, Loss: 0.004983039107173681\n",
      "Epoch 98, Loss: 0.004818866495043039\n",
      "Epoch 99, Loss: 0.004667628090828657\n",
      "\n",
      "Model architecture: \n",
      " MLP(\n",
      "  (linear_1): Linear()\n",
      "  (linear_2): Linear()\n",
      "  (linear_3): Linear()\n",
      ")\n",
      "\n",
      " state dict:\n",
      " OrderedDict([('linear_1.weight', Tensor(value=[[ 0.26285794  0.78290857 -0.18701821]\n",
      " [ 0.06489596  0.79706257 -0.43343713]\n",
      " [-0.22817197  0.81243849 -0.71707809]])), ('linear_1.bias', Tensor(value=[[ 0.30814937 -0.24283607 -0.69986419]])), ('linear_2.weight', Tensor(value=[[ 0.54201753  0.29891861  0.60520475]\n",
      " [ 0.06079862  0.24699491  0.79651862]\n",
      " [ 0.21161362  0.32837146 -0.16290384]\n",
      " [ 0.10945093  0.61775422  1.27667835]\n",
      " [ 0.3857235   0.54540256  0.89202707]\n",
      " [-0.36316576  0.29492704  0.93220842]])), ('linear_2.bias', Tensor(value=[[-0.06240454  0.48778918  0.83081246 -0.42450889 -0.25562898  0.47004975]])), ('linear_3.weight', Tensor(value=[[ 0.59065753  0.12257264 -0.15086139  0.95758111  0.29691619  0.76018893]])), ('linear_3.bias', Tensor(value=[[-0.3393324]]))])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. MiniTorch vs. PyTorch: Validation \n",
    "\n",
    "### Example 1: Simple Linear Case"
   ],
   "id": "86616868e30df8d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T21:51:11.839677Z",
     "start_time": "2024-12-18T21:51:11.786374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mse_loss_torch = torch_nn.MSELoss(reduction='sum')\n",
    "mse_loss_mini = mini_nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "a_torch = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
    "b_torch = torch.tensor([[5, 6], [7, 8]], requires_grad=True, dtype=torch.float32)\n",
    "y_torch = torch.tensor([[9, 2], [3, -1]], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "y_hat_torch = TorchF.linear(a_torch, b_torch)\n",
    "loss_torch = mse_loss_torch(y_hat_torch, y_torch)\n",
    "loss_torch.backward()\n",
    "\n",
    "a_mini = MiniTensor([[1, 2], [3, 4]])\n",
    "b_mini = MiniTensor([[5, 6], [7, 8]])\n",
    "y_mini = MiniTensor([[9, 2], [3, -1]])\n",
    "\n",
    "y_hat_mini = MiniF.linear(a_mini, b_mini)\n",
    "loss_mini = mse_loss_mini(y_hat_mini, y_mini)\n",
    "loss_mini.backward()\n",
    "\n",
    "\n",
    "gradients_are_equal(a_torch, a_mini) # compare d_loss/d_a\n",
    "gradients_are_equal(b_torch, b_mini) # compare d_loss/d_b"
   ],
   "id": "e23f214e9a9c9c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 2: More sophisticated case",
   "id": "1214fff0c3705f93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T21:51:12.196105Z",
     "start_time": "2024-12-18T21:51:12.151055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_a = [\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 10],\n",
    "    [9, -1, 1, 1]\n",
    "]\n",
    "\n",
    "val_b = [\n",
    "    [1, -2, 3, 4],\n",
    "    [4, 4, -6, 10],\n",
    "    [1, -1, 0, 1]\n",
    "]\n",
    "val_q = [\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "a_torch = torch.tensor(val_a, requires_grad=True, dtype=torch.float32)\n",
    "b_torch = torch.tensor(val_b, requires_grad=True, dtype=torch.float32)\n",
    "q_torch = torch.tensor(val_q, requires_grad=True, dtype=torch.float32)\n",
    "c_torch = TorchF.linear(a_torch, b_torch); c_torch.retain_grad()\n",
    "d_torch = TorchF.linear(c_torch, q_torch); d_torch.retain_grad()\n",
    "e_torch = d_torch.sum(); e_torch.retain_grad()\n",
    "e_torch.backward()\n",
    "\n",
    "\n",
    "a_mini = MiniTensor(val_a)\n",
    "b_mini = MiniTensor(val_b)\n",
    "c_mini = MiniF.linear(a_mini, b_mini)\n",
    "q_mini = MiniTensor(val_q)\n",
    "d_mini = MiniF.linear(c_mini, q_mini)\n",
    "e_mini = d_mini.sum()\n",
    "e_mini.backward()\n",
    "\n",
    "\n",
    "\n",
    "gradients_are_equal(a_torch, a_mini) # compare d_e/d_a\n",
    "gradients_are_equal(b_torch, b_mini) # compare d_e/d_b\n",
    "gradients_are_equal(c_torch, c_mini) # compare d_e/d_c\n",
    "gradients_are_equal(d_torch, d_mini) # compare d_e/d_d\n",
    "gradients_are_equal(e_torch, e_mini) # compare d_e/d_e\n",
    "gradients_are_equal(q_torch, q_mini) # compare d_e/d_q\n"
   ],
   "id": "dcbd8dbdff7fa64d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 3: High Dimensional Case",
   "id": "4e18577918c5dc0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T21:56:52.936190Z",
     "start_time": "2024-12-18T21:56:52.836727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_a = [\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3, 4]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [1, 9, -1, 4]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3, -1]\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "val_b = [\n",
    "    [1,   9,   3,   4],\n",
    "    [0,   1,  -1, -11],\n",
    "    [1,  21,  11,  -1]\n",
    "]\n",
    "\n",
    "a_torch = torch.tensor(val_a, requires_grad=True, dtype=torch.float32)\n",
    "b_torch = torch.tensor(val_b, requires_grad=True, dtype=torch.float32)\n",
    "c_torch = TorchF.linear(a_torch, b_torch)\n",
    "d_torch = c_torch.sum(); d_torch.retain_grad()\n",
    "d_torch.backward()\n",
    "\n",
    "a_mini = MiniTensor(val_a)\n",
    "b_mini = MiniTensor(val_b)\n",
    "c_mini = MiniF.linear(a_mini, b_mini)\n",
    "d_mini = c_mini.sum()\n",
    "d_mini.backward()\n",
    "\n",
    "gradients_are_equal(a_torch, a_mini) # compare d_d/d_a\n",
    "gradients_are_equal(b_torch, b_mini) # compare d_d/d_b"
   ],
   "id": "1bdb2f6afa8431b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
